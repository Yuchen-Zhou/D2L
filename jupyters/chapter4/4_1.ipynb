{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 多层感知机\n",
    "### 4.1.1 隐藏层\n",
    "在之前的章节里描述了仿射变换，它是一种带有偏置项的线性变化。首先，回顾一下softmax回归的模型架构。该模型通过单个仿射变换将我们的输入直接映射到输出，然后进行softmax操作。如果我们的标签通过仿射变换确实与我们的输入数据相关，那么这种方法确实足够了。但是，仿射变换中的线性是一个很强的假设。\n",
    "\n",
    "\n",
    "**线性模型可能会出错**\n",
    "\n",
    "例如，线性意味着单调假设：任何特征的增⼤都会导致模型输出的增⼤（如果对应的权重为正），或者导致模型输出的减⼩（如果对应的权重为负）。有时这是有道理的。例如，如果我们试图预测⼀个⼈是否会偿还贷款。我们可以认为，在其他条件不变的情况下，收⼊较⾼的申请⼈⽐收⼊较低的申请⼈更有可能偿还贷款。但是，虽然收⼊与还款率存在单调性，但它们不是线性相关的。收⼊从0增加到5万，可能⽐从100万增加到105万带来更⼤的还款可能性。处理这⼀问题的⼀种⽅法是对我们的数据进⾏预处理，使线性变得更合理，如使⽤收⼊的对数作为我们的特征。\n",
    "\n",
    "然而我们可以很容易找出违反单调性的例子。例如，我们想要根据体温预测死亡率。对于体温高于37度的人来说，温度越高风险越大。然而对于低于37度的人来说，温度越高风险越低。在这种情况下，我们也可以通过一些巧妙的预处理来解决问题。例如，我们可以使用与37度的距离作为特征。\n",
    "\n",
    "但是，如何对猫和狗的图像进行分类呢？增加位置(13, 17)处像素的强度是否总是增加(或降低)图像描绘狗的似然？对线性模型的依赖对应于一个隐含的假设，即区分猫和狗的唯一要求是评估单个像素的强度。对深度神经网络，我们使用观测数据来联合学习隐藏层表示和应用于该表示的线性预测器。\n",
    "\n",
    "**在网络中加入隐藏层** \n",
    "我们可以通过在网络中加入一个或多个隐藏层来克服线性模型的限制，使其能处理更普遍的函数关系类型。我们需要将许多全连接层堆叠在一起。每一层都输出到上面的层，直到生成最后的输出。我们可以把前$L-1$层看作表示，把最后一层看作线性预测器。这种架构通常称为多层感知机(multilayer perceptron)，通常写为*MLP*。下面，我们以图的方式描述了多层感知机。\n",
    "\n",
    "<div align=center>\n",
    "<img src='../../pics/4_1_1.jpeg' width='60%'>\n",
    "</div>\n",
    "\n",
    "这个多层感知机有4个输入，3个输出，其隐藏层包含5个隐藏单元。输入层不涉及任何计算，因此使用此网络产生输出只需要实现隐藏层和输出层中的计算神经元，而隐藏层中的每个神经元又会影响到输出层中的每个神经元。\n",
    "\n",
    "**从线性到非线性**\n",
    "\n",
    "我们通过矩阵$X \\in \\mathbb R^{n \\times d}$来表示$n$个样本的小批量，其中每个样本具有$d$个输入特征。对于具有$h$个隐藏单元的单隐藏层多层感知机，用$H \\in \\mathbb R^{n \\times h}$表示隐藏层的输出，称为隐藏表示(hidden representations)。在数学或代码中，$H$也被称为隐藏层变量(hidden-layer variable)。形式上，我们按如下方式计算单隐藏层多层感知机的输出$O \\in \\mathbb R^{n \\times q}$：\n",
    "$$\n",
    "H = XW^{(1)} + b^{(1)}\\\\\n",
    "O = HW^{(2)} + b^{(2)}\n",
    "$$\n",
    "\n",
    "为了发挥多层架构的潜力，我们还需要一个关键要素：在仿射变换之后对每个隐藏单元应用非线性的激活函数(activation function)$\\sigma$。激活函数的输出被称为活性值(activations)。一般来说，有了激活函数，就不可能再将我们的MLP退化成线性模型：\n",
    "$$\n",
    "H = \\sigma (XW^{(1)} + b^{(1)})\\\\\n",
    "O = HW^{(2)} + b^{(2)}\n",
    "$$\n",
    "\n",
    "**通用近似定理**  \n",
    "\n",
    "MLP可以通过隐藏神经元，捕捉到输入之间复杂的相互作用，这些神经元依赖于每个输入的值。我们可以很容易地设计隐藏节点来执行任意计算。例如，在一对输入上进行基本逻辑操作，多层感知机是通用近似器。即使网络只有一个隐藏层，给定足够的神经元和正确的权重，我们可以对任意函数建模，尽管实际中学习该函数是很困难的。\n",
    "\n",
    "而且，虽然一个单隐藏层网络能学习任何函数，但不意味着我们应该尝试使用单隐藏层网络来解决所有问题。实际上，使用更深的网络，我们可以更容易地逼近许多函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
